\documentclass[12pt, a4paper]{article} % set document type and sizes

%---------------------------------------------------------------------------------------------------------------------
% Packages
%---------------------------------------------------------------------------------------------------------------------

%-------------------------------------------------------
% Useful Packages.

\usepackage{amsmath} % prints mathematical formulas
\usepackage{enumitem} % handles lists

\usepackage{multirow} % handles merging cells in tables
\usepackage{float} % adds [H] option to \begin{table}[H] to restrict floating.
% to import tables from excel and csv use http://www.tablesgenerator.com/latex_tables

\usepackage{cite} % Bibliography support 

% For Greek characters support compile with XeLaTeX and include
%\usepackage{xltxtra} % Greek support
%\usepackage{xgreek} % Greek support
%\setmainfont[Mapping=tex-text]{Garamond} % Font choice

\usepackage{listings} % To insert formatted code
\usepackage{color} % To color text

\usepackage{algpseudocode} % To insert algorithms (both needed)
\usepackage{algorithm} % To insert algorithms (both needed)

%---------------------------------------------------------------------------------------------------------------------
% Code Format Settings
%---------------------------------------------------------------------------------------------------------------------

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

%-------------------------------------------------------------------------------------------------------------------


%---------------------------------------------------------------------------------------------------------------------
% Title Section
%---------------------------------------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % command for creating lines to place the title in a box

\title{	
\normalfont \normalsize 
\textsc{How GBM Machine learning algorithm works} \\ [25pt] % University name and department
\horrule{0.5pt} \\[0.4cm] % Top line
\huge Gradient Boost\\ % The report title
\horrule{2pt} \\[0.5cm] % Bottom line
}

\author{} % Author's name

\date{\today} % Today's date

%---------------------------------------------------------------------------------------------------------------------
% Main Document
%---------------------------------------------------------------------------------------------------------------------

\begin{document}

\maketitle % print title

%---------------------------------------------------------------------------------------------------------------------
% Introduction
%---------------------------------------------------------------------------------------------------------------------

\section{Gradient Boost Regression}
Specifcally we will use this data:

\begin{table}[h!]
\begin{center}
\begin{tabular}{m|l|c|r} % <-- Alignments
    
    \hline
    \hline
    
         \textbf{Height(m)}
     
       & \textbf{Favorite Color}
      
       & \textbf{Gender}
       
       & \textbf{Weight(kg)}
       
      \\
      
      \hline
      \hline
      
      1.6 & Blue  & Male   & 88\\
      1.6 & Green & Female & 76\\
      1.5 & Blue  & Female & 56\\
      1.8 & Red   & Male   & 73\\
      1.5 & Green & Male   & 77\\
      1.4 & Blue  & Female & 57\\
      
      \hline
      \hline
      
\end{tabular}
\end{center}
\end{table}

\paragraph{NOTE:} When \textbf{Gradient Boost} is used to predict a continuous value, like \textbf{Weight(kg)}, we say that we are using \textbf{Gradient Boost} for \textbf{Regression}.Using \textbf{Gradient Boost} for \textbf{Regression} is different from doing \textbf{Linear Regression}, so even though two methods are related, we do not ge them confused with each other.


\subsection{AdaBoost and Gradient Boost}

If we using these measurements to \textbf{Predict Weight} then \textbf{Adaboost} starts by building a very short tree, called a \textbf{Stump}, from the \textbf{Training Data}. The amount of say that the stump has on the final output is based on how well it compensated for those previous errors. Then \textbf{Adaboost} builds the next stump based on errors that the previous stump made.

\newline

In contrast, \textbf{Gradient Boost} starting by making a single leaf, instead of tree or stump. This leaf represents the initial guess for the \textbf{Weights} of all of the samples. When trying to predict a continuous value like \textbf{Weight} in our example, the first guess is the average value.

\begin{align*}
    \overline{Weight(kg)} = \frac{88 + 76 + 56 + 73 + 77 + 57}{6} = \textbf{71.2}\\
\end{align*}

\textbf{But}, unlike \textbf{AdaBoost}, this tree is usually larger than a stump.
That said, \textbf{Gradient Boost} still restricts the size of the tree. In this simple example - for educational purposes - , will be built tress with up to four leaves but no larger.

\paragraph{Thus}, however in practice, people often set the \textbf{maximum number of leaves} to be between \textbf{8} and \textbf{32}. Like \textbf{AdaBoost}, \textbf{GradientBoost} builds fixed sized trees based on the previous tree's errors, but using \texbf{Gradient Boost} each tree can be larger than a stump. Also, like \textbf{AdaBoost}, \texbf{Gradient Boost} scales all trees.However \textbf{Gradient} scales all treess by the same amount.

     
     
\subsection{Most Common Gradient Boost Configuration}

\paragraph{The first thing we did} is to calculate the average \textbf{Weight}. This is the first attempt at predicting someone's weight. However, \texbf{Gradient Boost} does not stop here.

\begin{align*}
    \overline{Weight(kg)} = \frac{88 + 76 + 56 + 73 + 77 + 57}{6} = \textbf{71.2}\\
\end{align*}

\paragraph{The next thing we do} is to build a tree based on the errors from the first tree. The errors that the previous tree made are the differences between the \tetxbf{The Observed Weights} and the \textbf{Predicted Weights}.So let's start by plugging in \textbf{71.2} for the \textbf{Predicted Weights}.

Specifically, \textbf{(Observed Weight - Predicted Weight):}



\begin{table}[h!]
\begin{center}
\begin{tabular}{c|r} % <-- Alignments
    
    \hline
    \hline
       
        \textbf{Observed Weight(kg)}
        
    &   \textbf{Predicted Weight(kg)}
       
      \\
      
      \hline
      \hline
      
       88 &  16.8\\
       76 &  4.8\\
       56 &  -15.2 \\
       73 &  1.8\\
       77 &  5.8\\
       57 &  -14.2\\
      
      \hline
      \hline
      
\end{tabular}
\end{center}
\end{table}

\textbf{HINT:} If it seems strange to predict the residuals instead of the original Weights, just bear with me and soon all will become clear.

\paragraph{Now, we proceed building a tree -PyCode-}:



\begin{lstlisting}

import pandas as pd
import numpy as np

#Calculating Difference Between Observed Values and Predicted(Average as single leaf)

residuals = []
average_weight = np.mean(data['Weight'])
for values in data['Weight']:
    residual_value = (values - average_weight)
    residuals.append(residual_value)
    
#Assign Residuals in Pandas DataFrame
data['residuals'] = residuals
\end{lstlisting}



\textbf{So, setting aside the reason why we are bulding a tree to Predict the Residuals for the time being, here's the tree -codely and visually-.}


\begin{lstlisting}

#Lets build a tree with

for values in data[['Gender', 'Height', 'residuals', 'Favorite_Color']].values:
    if   values[0] == 'Female' and values[1] < 1.6:
        print(f"Female and smallest than 1.6    : {values[2]}.")
    elif values[0] == 'Female' and values[1] > 1.6:
        print(f"Female and tallest than 1.6     : {values[2]}.")
    elif values[0] != 'Female' and values[3] != 'Blue':
        print(f"Male who do not like blue color : {values[2]}.")
    elif values[0] != 'Female' and values[3] == 'Blue':
        print(f"Male with favorite color blue   : {values[2]}.")
    else:
        print('--')

\end{lstlisting}


 The initial tree(-code-) has been built as below:
 
\begin{itemize}

\item Thus, if observation is female and her height is \textbf{lower} than 1.6(m) then the residuals are -14.2 and -15.2, so taking the average of these two rows of data go to the same leaf and replace them, i.e $\frac{-14.2 + (-15.2)}{2} = -14.7$.

\item If observation is Female and her height is \textbf{taller} than 1.6(m), then the row with residual 4.8 has been assigned in the second leaf.

\item If Observation is not Female(i.e is Male) and his favorite color is not Blue then in the third leaf has been assigned two rows with residuals 1.8 and 5.8.So we replace it with their average ${1.8 + 5.8}{2} = 3.8$.

\item If Observation is Male nad his favorite color is Blue, then the row with residual 16.8 is assigned in the fourth leaf.

\end{itemize}


Now, we can combine the original leaf with the new tree to make a new prediction of an individual's \textbf{Weight} from the \textbf{Training Data}.
We start with the initial prediction. We should remember that the initial prediction is \tetxbf{71.2}, i.e the average of all observations. Then, for the observation with these values(\tetxbf{Weight = 1.6, Favorite Color = Blue, Gender = Male, Weight} we run the data down the tree and we get 16.8. So the \textbf{Predicted Weight is 71.2 + 16.8 = 88, which is the same as the Observed Weight.} Is this awesome? \textbf{NO!!!!} The model fits the Training Data too well. In other words, we have low bias, but probably very high variance.

\newline

\textbf{Gradient Boost} deals with this problem by using a \textbf{Learning rate}, to scale the contribution of the new tree. The learning rate is a value between 0 and 1. In this case, we will set the learning rate to 0.1. Thus, now the \textbf{Predicted weight} is \textbf{71.2 + (0.1 x 16.8) = which equals 72.9}.As we can see setting learning rate , the prediction is not as good it was before but it is a little better than the Prediction made with just the original leaf, i.e 71.2 the average of all observations. In other words, scaling the tree by the \textbf{the Learning Rate} results in a small step in the right direction. According to \textbf{Jerome Friedman}, empirical evidence shows that taking lots of small steps in the right direction results in better Predictions with a Testing Dataset, i.e lower variance.

\newline

So, just like before we are trying to build a new tree, calculating \textbf{Pseudo Residuals}, i.e the difference between the observed weights and the latest Predictions.So we plug in the Observed Weight and the new Predicted Weight : 

\begin{align*}
    & Residuals = (Observed -   (Initial\_Prediction + Learning\_Rate \times  Residual\_in\_the\_ leaf)) 
\end{align*}

So, we save that in the column of \textbf{Pseudo Residuals}, repeating all of the training individuals in the training set. Specifically:

\begin{enumerate}
    \item 15.1
    \item 4.3
    \item -13.7
    \item 1.4
    \item 5.4
    \item 12.7
\end{enumerate}

\paragraph{NOTE:} The original \textbf{Residuals} are these from the initial Prediction, using simply the average overall observations and Weights. So, the residuals that above have been calculated, after adding the new scaled tree by the \textbf{Learning Rate} are \textbf{Residuals}. Please, keep in mind the procedure! As you can perceive, the new Residuals are smaller than before, so we have taken a small step in the right direction.

\newline

So, the second procedure is repeated, building a new tree to predict the new Residuals. The procedure is the same as above but the residuals of the second trial are assigned in each leaf.\textbf{Note :} Here, for educational purposes the brances are the same as before!

\textbf{So, in this step we combine the new tree with the prvious tree and the initial Leaf, scaling each tree by the Learning Rate, which we set to 0.1}. Thus, add the scaled amount from the first tree and the scaled amount from the second tree. Another small step closer to the Observed Weight.


\paragraph{HINT :} We keep making trees until we reach the maximum specified, or adding additional trees does not significantly reduce the size of the \textbf{}{Residuals}.

\paragraph{In summary, when Gradient Boost is used for Regression}:

\begin{enumerate}

    \item We start with a leaf which is the average of target variable of all observations. In this case, \textbf{Weight}.
    
    \item Then we add a tree based on the \textbf{Residuals}, the difference between the \textbf{Observed} and \textbf{Predicted} values and we scale the tree's contribution to the final predictions with a \textbf{Learning Rate}.
    
    \item Then we add another tree based on the \textbf{Residuals}...
    
    \item ...and we keep adding trees based on the errors made by the previous tree.
\end{enumerate}


%---------------------------------------------------------------------------------------------------------------------
% Section 1
%---------------------------------------------------------------------------------------------------------------------

\section{Mathematics behind the Gradient Boost algorithm for Regression}


Now we are going to dive into some regression details. In this chapter we are going to walk through the original \textbf{Gradient Boost} algorithm step by step.
In order to keep the example from getting out of hand, we are going to use \textbf{Gradient Boost} to fit a model to a super simple \textbf{Training Dataset}. Sepcifically :


\begin{table}[h!]
\begin{center}
\begin{tabular}{m|l|c|r} % <-- Alignments
    
    \hline
    \hline
    
         \textbf{Height(m)}
     
       & \textbf{Favorite Color}
      
       & \textbf{Gender}
       
       & \textbf{Weight(kg)}
       
      \\
      
      \hline
      \hline
      
      1.6 & Blue  & Male   & 88\\
      1.6 & Green & Female & 76\\
      1.5 & Blue  & Female & 56\\
      
      \hline
      \hline
      
\end{tabular}
\end{center}
\end{table}

Now that we know all about this super simple Training Dataset, let's walk through the original \textbf{Gradient Descent} algorithm step by step.

\subsection{Starting from the top}

\paragraph{Input:} Data
\begin{align*}
    (x_{i}, y_{i})^n_{i = 1}
\end{align*}

The above line describes, in an abstract way, the Training dataset and the method we will use to evaluate how well the model fits the Training dataset. Thus, this part is all about a way to say that we have some data. 


The $x_{i}$'s refers to each row of measurements that we will use to predict \textbf{Weight}.


The $y_{i}$'s refer to the weights measured for each person in the dataset.

The part $^n_{i = 1}$ just says that the i's, in $x_{i}$ and $y_{i}$ go from \textbf{1} to \textbf{n}, where n is the number of people in the dataset. In this case \textbf{n = 3}, since we have only 3 rows of data.


\paragraph{Differentiable Loss Function:}
\begin{align*}
    \textbf{Loss Function} L(y_{i}, F(x))
\end{align*}

Now we need a differentiable loss function. In this case a loss function is something that evaluates how well we can predict Weight. The most commonly used when we are doing\fe regression with Gradient Boost is:
\begin{align*}
    \frac{1}{2}(Observed - Predicted)^2
\end{align*}

Now, just for a few minutes, ignore the $\frac{1}{2}$ in front. As you can, removing 1/2 we have the same loss function as in Linear Regression.

For example, if we have Height and we want to predict Weight. Thus, the loss function is just a squared residual. Specifically, let's say that we have been fitted two lines. For the first sum of squared residuals is 0.26, but for the second line sum of squared Residuals is 6.43. Obviously, the 0.26 is smaller than the second line and 6.43. If i multiplied both sides of the formulas by $\frac{1}{2}$ and do the math we have smaller numbers but we still know that the first line fits the data relatively better in contrast with the second line, is still relatively smaller the sum of squared residuals.

In other words, it does not matter if the loss function has been multiplied by $\frac{1}{2}$.
However, the reason that people choose this loss function $\frac{1}{2}(Observed - Predicted)^2$ for Gradient Boost is due to the fact that when we differentiate it with respect to Predicted, i.e:
\begin{align*}
    \frac{\partial}{\partial Predicted} \frac{1}{2}(Observed - Predicted)^2
\end{align*}

using the \textbf{CHAIN RULE} and bring the square down $\frac{2}{2}(Observed - Predicted)$ to the front and multiply it by the derivative of -Predicted, which is -1 $\frac{2}{2}(Observed - Predicted) \times -1$, then the $\frac{2}{2}$ cancels out and that leave us with $-(Obsrved - Predicted)$. In other words, we are left with the negative Residual, and that makes the math easier since \textbf{Gradient Boost} uses the derivative a lot.
%---------------------------------------------------------------------------------------------------------------------
% Tables
%---------------------------------------------------------------------------------------------------------------------

\subsection{Tables}

% You can write the code of your table by hand but i highly recommend using a tool like http://www.tablesgenerator.com/latex_tables

\begin{table}[H]
\centering
\caption{A Table}
\label{tab:tbl1}
\begin{tabular}{llllllllll}
\textbf{L}                      & \textbf{o}             & \textbf{r}             & \textbf{e}             & \textbf{m}             & \textbf{i}             & \textbf{p}             & \textbf{s}             & \textbf{u}             & \textbf{m}             \\ \cline{2-10} 
\multicolumn{1}{l|}{\textbf{d}} & \multicolumn{1}{l|}{o} & \multicolumn{1}{l|}{l} & \multicolumn{1}{l|}{o} & \multicolumn{1}{l|}{r} & \multicolumn{1}{l|}{s} & \multicolumn{1}{l|}{i} & \multicolumn{1}{l|}{t} & \multicolumn{1}{l|}{a} & \multicolumn{1}{l|}{m} \\ \cline{2-10} 
\multicolumn{1}{l|}{\textbf{e}} & \multicolumn{1}{l|}{t} & \multicolumn{1}{l|}{,} & \multicolumn{1}{l|}{c} & \multicolumn{1}{l|}{o} & \multicolumn{1}{l|}{n} & \multicolumn{1}{l|}{s} & \multicolumn{1}{l|}{e} & \multicolumn{1}{l|}{c} & \multicolumn{1}{l|}{t} \\ \cline{2-10} 
\end{tabular}
\end{table}

%---------------------------------------------------------------------------------------------------------------------
% List
%---------------------------------------------------------------------------------------------------------------------

\subsection{List}

\begin{itemize}

\item

\item

\item


\end{itemize}

%---------------------------------------------------------------------------------------------------------------------
% Math
%---------------------------------------------------------------------------------------------------------------------

\subsection{Math}

% This should be the best guide available for using math in LaTeX:
% ftp://ftp.ams.org/pub/tex/doc/amsmath/short-math-guide.pdf

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. 

\begin{equation} %begin equation
 \sum_{\substack{ % equation math, follow the aforementioned guide
		 i \in S}}
	S_i
\end{equation} % end equation

Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. \(p_(t1)=\theta^(-\frac{\Delta}{s})\) % inline math
Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

%---------------------------------------------------------------------------------------------------------------------
% Code
%---------------------------------------------------------------------------------------------------------------------
\newpage
\subsection{Code Example}


 

%---------------------------------------------------------------------------------------------------------------------
% Algorithm
%---------------------------------------------------------------------------------------------------------------------
\newpage
\subsection{Algorithm}



\begin{algorithm}
\caption{Lorem ipsum dolor sit amet}
\begin{algorithmic}



\end{algorithmic}
\end{algorithm}

%---------------------------------------------------------------------------------------------------------------------
% Conclusions
%---------------------------------------------------------------------------------------------------------------------

\section{Conclusions}





%---------------------------------------------------------------------------------------------------------------------
% Bibliography
%---------------------------------------------------------------------------------------------------------------------

\newpage
\bibliography{Report-Template} % .bib file name. Do not add ".bib"
\bibliographystyle{IEEEtr} % Bibliography style. Find more styles at
% https://www.sharelatex.com/learn/Bibtex_bibliography_styles

%If you include bibliography without a single reference inside the document, the TeX file will not compile.

\end{document}