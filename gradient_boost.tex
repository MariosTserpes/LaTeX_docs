\documentclass[12pt, a4paper]{article} % set document type and sizes

%---------------------------------------------------------------------------------------------------------------------
% Packages
%---------------------------------------------------------------------------------------------------------------------

%-------------------------------------------------------
% Useful Packages.

\usepackage{amsmath} % prints mathematical formulas
\usepackage{enumitem} % handles lists

\usepackage{multirow} % handles merging cells in tables
\usepackage{float} % adds [H] option to \begin{table}[H] to restrict floating.
% to import tables from excel and csv use http://www.tablesgenerator.com/latex_tables

\usepackage{cite} % Bibliography support 

% For Greek characters support compile with XeLaTeX and include
%\usepackage{xltxtra} % Greek support
%\usepackage{xgreek} % Greek support
%\setmainfont[Mapping=tex-text]{Garamond} % Font choice

\usepackage{listings} % To insert formatted code
\usepackage{color} % To color text

\usepackage{algpseudocode} % To insert algorithms (both needed)
\usepackage{algorithm} % To insert algorithms (both needed)

%---------------------------------------------------------------------------------------------------------------------
% Code Format Settings
%---------------------------------------------------------------------------------------------------------------------

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

%-------------------------------------------------------------------------------------------------------------------


%---------------------------------------------------------------------------------------------------------------------
% Title Section
%---------------------------------------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % command for creating lines to place the title in a box

\title{	
\normalfont \normalsize 
\textsc{How GBM Machine learning algorithm works} \\ [25pt] % University name and department
\horrule{0.5pt} \\[0.4cm] % Top line
\huge Gradient Boost\\ % The report title
\horrule{2pt} \\[0.5cm] % Bottom line
}

\author{} % Author's name

\date{\today} % Today's date

%---------------------------------------------------------------------------------------------------------------------
% Main Document
%---------------------------------------------------------------------------------------------------------------------

\begin{document}

\maketitle % print title

%---------------------------------------------------------------------------------------------------------------------
% Introduction
%---------------------------------------------------------------------------------------------------------------------

\section{Gradient Boost for Regression}
Specifcally we will use this data:

\begin{table}[h!]
\begin{center}
\begin{tabular}{m|l|c|r} % <-- Alignments
    
    \hline
    \hline
    
         \textbf{Height(m)}
     
       & \textbf{Favorite Color}
      
       & \textbf{Gender}
       
       & \textbf{Weight(kg)}
       
      \\
      
      \hline
      \hline
      
      1.6 & Blue  & Male   & 88\\
      1.6 & Green & Female & 76\\
      1.5 & Blue  & Female & 56\\
      1.8 & Red   & Male   & 73\\
      1.5 & Green & Male   & 77\\
      1.4 & Blue  & Female & 57\\
      
      \hline
      \hline
      
\end{tabular}
\end{center}
\end{table}

\paragraph{NOTE:} When \textbf{Gradient Boost} is used to predict a continuous value, like \textbf{Weight(kg)}, we say that we are using \textbf{Gradient Boost} for \textbf{Regression}.Using \textbf{Gradient Boost} for \textbf{Regression} is different from doing \textbf{Linear Regression}, so even though two methods are related, we do not ge them confused with each other.


\subsection{AdaBoost and Gradient Boost}

If we using these measurements to \textbf{Predict Weight} then \textbf{Adaboost} starts by building a very short tree, called a \textbf{Stump}, from the \textbf{Training Data}. The amount of say that the stump has on the final output is based on how well it compensated for those previous errors. Then \textbf{Adaboost} builds the next stump based on errors that the previous stump made.

\newline

In contrast, \textbf{Gradient Boost} starting by making a single leaf, instead of tree or stump. This leaf represents the initial guess for the \textbf{Weights} of all of the samples. When trying to predict a continuous value like \textbf{Weight} in our example, the first guess is the average value.

\begin{align*}
    \overline{Weight(kg)} = \frac{88 + 76 + 56 + 73 + 77 + 57}{6} = \textbf{71.2}\\
\end{align*}

\textbf{But}, unlike \textbf{AdaBoost}, this tree is usually larger than a stump.
That said, \textbf{Gradient Boost} still restricts the size of the tree. In this simple example - for educational purposes - , will be built tress with up to four leaves but no larger.

\paragraph{Thus}, however in practice, people often set the \textbf{maximum number of leaves} to be between \textbf{8} and \textbf{32}. Like \textbf{AdaBoost}, \textbf{GradientBoost} builds fixed sized trees based on the previous tree's errors, but using \texbf{Gradient Boost} each tree can be larger than a stump. Also, like \textbf{AdaBoost}, \texbf{Gradient Boost} scales all trees.However \textbf{Gradient} scales all treess by the same amount.

     
     
\subsection{Most Common Gradient Boost Configuration}

\paragraph{The first thing we did} is to calculate the average \textbf{Weight}. This is the first attempt at predicting someone's weight. However, \texbf{Gradient Boost} does not stop here.

\begin{align*}
    \overline{Weight(kg)} = \frac{88 + 76 + 56 + 73 + 77 + 57}{6} = \textbf{71.2}\\
\end{align*}

\paragraph{The next thing we do} is to build a tree based on the errors from the first tree. The errors that the previous tree made are the differences between the \tetxbf{The Observed Weights} and the \textbf{Predicted Weights}.So let's start by plugging in \textbf{71.2} for the \textbf{Predicted Weights}.

Specifically, \textbf{(Observed Weight - Predicted Weight):}



\begin{table}[h!]
\begin{center}
\begin{tabular}{c|r} % <-- Alignments
    
    \hline
    \hline
       
        \textbf{Observed Weight(kg)}
        
    &   \textbf{Predicted Weight(kg)}
       
      \\
      
      \hline
      \hline
      
       88 &  16.8\\
       76 &  4.8\\
       56 &  -15.2 \\
       73 &  1.8\\
       77 &  5.8\\
       57 &  -14.2\\
      
      \hline
      \hline
      
\end{tabular}
\end{center}
\end{table}

\textbf{HINT:} If it seems strange to predict the residuals instead of the original Weights, just bear with me and soon all will become clear.

\paragraph{Now, we proceed building a tree -PyCode-}:



\begin{lstlisting}

import pandas as pd
import numpy as np

#Calculating Difference Between Observed Values and Predicted(Average as single leaf)

residuals = []
average_weight = np.mean(data['Weight'])
for values in data['Weight']:
    residual_value = (values - average_weight)
    residuals.append(residual_value)
    
#Assign Residuals in Pandas DataFrame
data['residuals'] = residuals
\end{lstlisting}



\textbf{So, setting aside the reason why we are bulding a tree to Predict the Residuals for the time being, here's the tree -codely and visually-.}


\begin{lstlisting}

#Lets build a tree with

for values in data[['Gender', 'Height', 'residuals', 'Favorite_Color']].values:
    if   values[0] == 'Female' and values[1] < 1.6:
        print(f"Female and smallest than 1.6    : {values[2]}.")
    elif values[0] == 'Female' and values[1] > 1.6:
        print(f"Female and tallest than 1.6     : {values[2]}.")
    elif values[0] != 'Female' and values[3] != 'Blue':
        print(f"Male who do not like blue color : {values[2]}.")
    elif values[0] != 'Female' and values[3] == 'Blue':
        print(f"Male with favorite color blue   : {values[2]}.")
    else:
        print('--')

\end{lstlisting}


 The initial tree(-code-) has been built as below:
 
\begin{itemize}

\item Thus, if observation is female and her height is \textbf{lower} than 1.6(m) then the residuals are -14.2 and -15.2, so taking the average of these two rows of data go to the same leaf and replace them, i.e $\frac{-14.2 + (-15.2)}{2} = -14.7$.

\item If observation is Female and her height is \textbf{taller} than 1.6(m), then the row with residual 4.8 has been assigned in the second leaf.

\item If Observation is not Female(i.e is Male) and his favorite color is not Blue then in the third leaf has been assigned two rows with residuals 1.8 and 5.8.So we replace it with their average ${1.8 + 5.8}{2} = 3.8$.

\item If Observation is Male nad his favorite color is Blue, then the row with residual 16.8 is assigned in the fourth leaf.

\end{itemize}


Now, we can combine the original leaf with the new tree to make a new prediction of an individual's \textbf{Weight} from the \textbf{Training Data}.
We start with the initial prediction. We should remember that the initial prediction is \tetxbf{71.2}, i.e the average of all observations. Then, for the observation with these values(\tetxbf{Weight = 1.6, Favorite Color = Blue, Gender = Male, Weight} we run the data down the tree and we get 16.8. So the \textbf{Predicted Weight is 71.2 + 16.8 = 88, which is the same as the Observed Weight.} Is this awesome? \textbf{NO!!!!} The model fits the Training Data too well. In other words, we have low bias, but probably very high variance.

\newline

\textbf{Gradient Boost} deals with this problem by using a \textbf{Learning rate}, to scale the contribution of the new tree. The learning rate is a value between 0 and 1. In this case, we will set the learning rate to 0.1. Thus, now the \textbf{Predicted weight} is \textbf{71.2 + (0.1 x 16.8) = which equals 72.9}.As we can see setting learning rate , the prediction is not as good it was before but it is a little better than the Prediction made with just the original leaf, i.e 71.2 the average of all observations. In other words, scaling the tree by the \textbf{the Learning Rate} results in a small step in the right direction. According to \textbf{Jerome Friedman}, empirical evidence shows that taking lots of small steps in the right direction results in better Predictions with a Testing Dataset, i.e lower variance.

\newline

So, just like before we are trying to build a new tree, calculating \textbf{Pseudo Residuals}, i.e the difference between the observed weights and the latest Predictions.So we plug in the Observed Weight and the new Predicted Weight : 

\begin{align*}
    & Residuals = (Observed -   (Initial\_Prediction + Learning\_Rate \times  Residual\_in\_the\_ leaf)) 
\end{align*}

So, we save that in the column of \textbf{Pseudo Residuals}, repeating all of the training individuals in the training set. Specifically:

\begin{enumerate}
    \item 15.1
    \item 4.3
    \item -13.7
    \item 1.4
    \item 5.4
    \item 12.7
\end{enumerate}

\paragraph{NOTE:} The original \textbf{Residuals} are these from the initial Prediction, using simply the average overall observations and Weights. So, the residuals that above have been calculated, after adding the new scaled tree by the \textbf{Learning Rate} are \textbf{Residuals}. Please, keep in mind the procedure! As you can perceive, the new Residuals are smaller than before, so we have taken a small step in the right direction.

\newline

So, the second procedure is repeated, building a new tree to predict the new Residuals. The procedure is the same as above but the residuals of the second trial are assigned in each leaf.\textbf{Note :} Here, for educational purposes the brances are the same as before!

\textbf{So, in this step we combine the new tree with the prvious tree and the initial Leaf, scaling each tree by the Learning Rate, which we set to 0.1}. Thus, add the scaled amount from the first tree and the scaled amount from the second tree. Another small step closer to the Observed Weight.


\paragraph{HINT :} We keep making trees until we reach the maximum specified, or adding additional trees does not significantly reduce the size of the \textbf{}{Residuals}.

\paragraph{In summary, when Gradient Boost is used for Regression}:

\begin{enumerate}

    \item We start with a leaf which is the average of target variable of all observations. In this case, \textbf{Weight}.
    
    \item Then we add a tree based on the \textbf{Residuals}, the difference between the \textbf{Observed} and \textbf{Predicted} values and we scale the tree's contribution to the final predictions with a \textbf{Learning Rate}.
    
    \item Then we add another tree based on the \textbf{Residuals}...
    
    \item ...and we keep adding trees based on the errors made by the previous tree.
\end{enumerate}


%---------------------------------------------------------------------------------------------------------------------
% Section 1
%---------------------------------------------------------------------------------------------------------------------

\section{Mathematics behind the Gradient Boost algorithm for Regression}


Now we are going to dive into some regression details. In this chapter we are going to walk through the original \textbf{Gradient Boost} algorithm step by step.
In order to keep the example from getting out of hand, we are going to use \textbf{Gradient Boost} to fit a model to a super simple \textbf{Training Dataset}. Sepcifically :


\begin{table}[h!]
\begin{center}
\begin{tabular}{m|l|c|r} % <-- Alignments
    
    \hline
    \hline
    
         \textbf{Height(m)}
     
       & \textbf{Favorite Color}
      
       & \textbf{Gender}
       
       & \textbf{Weight(kg)}
       
      \\
      
      \hline
      \hline
      
      1.6 & Blue  & Male   & 88\\
      1.6 & Green & Female & 76\\
      1.5 & Blue  & Female & 56\\
      
      \hline
      \hline
      
\end{tabular}
\end{center}
\end{table}

Now that we know all about this super simple Training Dataset, let's walk through the original \textbf{Gradient Descent} algorithm step by step.

\subsection{Starting from the top}

\paragraph{Input:} Data
\begin{align*}
    (x_{i}, y_{i})^n_{i = 1}
\end{align*}

The above line describes, in an abstract way, the Training dataset and the method we will use to evaluate how well the model fits the Training dataset. Thus, this part is all about a way to say that we have some data. 


The $x_{i}$'s refers to each row of measurements that we will use to predict \textbf{Weight}.


The $y_{i}$'s refer to the weights measured for each person in the dataset.

The part $^n_{i = 1}$ just says that the i's, in $x_{i}$ and $y_{i}$ go from \textbf{1} to \textbf{n}, where n is the number of people in the dataset. In this case \textbf{n = 3}, since we have only 3 rows of data.


\paragraph{Differentiable Loss Function:}
\begin{align*}
    \textbf{Loss Function} L(y_{i}, F(x))
\end{align*}

Now we need a differentiable loss function. In this case a loss function is something that evaluates how well we can predict Weight. The most commonly used when we are doing\fe regression with Gradient Boost is:
\begin{align*}
    \frac{1}{2}(Observed - Predicted)^2
\end{align*}

Now, just for a few minutes, ignore the $\frac{1}{2}$ in front. As you can, removing 1/2 we have the same loss function as in Linear Regression.

For example, if we have Height and we want to predict Weight. Thus, the loss function is just a squared residual. Specifically, let's say that we have been fitted two lines. For the first sum of squared residuals is 0.26, but for the second line sum of squared Residuals is 6.43. Obviously, the 0.26 is smaller than the second line and 6.43. If i multiplied both sides of the formulas by $\frac{1}{2}$ and do the math we have smaller numbers but we still know that the first line fits the data relatively better in contrast with the second line, is still relatively smaller the sum of squared residuals.

In other words, it does not matter if the loss function has been multiplied by $\frac{1}{2}$.
However, the reason that people choose this loss function $\frac{1}{2}(Observed - Predicted)^2$ for Gradient Boost is due to the fact that when we differentiate it with respect to Predicted, i.e:
\begin{align*}
    \frac{\partial}{\partial Predicted} \frac{1}{2}(Observed - Predicted)^2
\end{align*}

using the \textbf{CHAIN RULE} and bring the square down $\frac{2}{2}(Observed - Predicted)$ to the front and multiply it by the derivative of -Predicted, which is -1 $\frac{2}{2}(Observed - Predicted) \times -1$, then the $\frac{2}{2}$ cancels out and that leave us with $-(Obsrved - Predicted)$. In other words, we are left with the negative Residual, and that makes the math easier since \textbf{Gradient Boost} uses the derivative a lot.


So, as you can see, we have a \textbf{Loss Function}:
\begin{align*}
    L(y_{i}, f(X)) =
    \frac{1}{2}(Observed - Predicted)^2
\end{align*}
The $y_{i}$ are the Observed Values and $f(X)$ is a function that gives us the \textbf{Predicted} values. We also know that the \textbf{Loss Function} is differentiable, since we have already taken the derivative, i.e:
\begin{align*}
    \frac{\partial}{\partial Predicted} \frac{1}{2}(Observed - Predicted)^2 = -(Observed - Predicted)
\end{align*}

We've figured out whay the inputs is for \textbf{Gradient Boost} algorithm. We have got data:
\begin{align*}
    (x_{i}, y_{i})^n_{i = 1}
\end{align*}

and we have got a differentiable loss function:
\begin{align*}
    \frac{1}{2}(Observed - Predicted)^2 
\end{align*}



\subsubsection{Step 1}

\paragraph{Step 1:} Initialize model with a constant value:
\begin{align*}
    F_{0}(X) = argmin \sum L(y_{i}, \gamma)
\end{align*}

We start by initializing the model with a constant value and the constant value is determined by the above equation. The above equation is easiest more understandable if we start on the right side and work our way left. As you can perceive the $L(y_{i}, \gamma)$ is just the loss function, i.e $\frac{1}{2}(Observed - Predicted)^2$. The $y_{i}$ refers to the Observed values and the $\gamma$ refers to the Predicted Values. The ($\sum$) summation that we add up one Loss Function for each Observed value. For example:
\begin{align*}
    \frac{1}{2}(88 - Predicted)^2 + \frac{1}{2}(76 - Predicted)^2 +
    \frac{1}{2}(56 - Predicted)^2 
\end{align*}

and the \textbf{argmin over $\gamma$} means that we need to find a Predicted value that minimizes the above sum. In other worlds, if we plot the Observed Weights on a number line, then we want to find the point on the line that minimizes the sum of the squared residuals diveded by 2. Of course, we can use \textbf{Gradient Descent} in order to find the optimal value that minimizes the sum of the squared residuals, but we also can solve it, due to the fact that the maths are not difficult procedure.

\paragraph{The first thing we do} is to take the derivative of each term with respect to Predicted and we set sum of the derivatives equal to zero. Specifically:
\begin{align*}
    \frac{\partial}{\partial Predicted}\frac{1}{2}(88 - Predicted)^2  +\\ \frac{\partial}{\partial Predicted}\frac{1}{2}(76 - Predicted)^2  +\\ \frac{\partial}{\partial Predicted} \frac{1}{2}(56 - Predicted)^2 \rightarrow\\
    -(88 - Predicted) + (-(76 - Predicted)) + (-(56 - Predicted)) = 0\\
\end{align*}

...and solve:
\begin{align*}
    & -(88 - Predicted) + (-(76 - Predicted)) +   (-(56 - Predicted)) = 0 \rightarrow\\
    & (-88 + Predicted) + (-76 + Predicted) +   (-56 + Predicted) = 0 \rightarrow\\
    & Predicted + Predicted + Predicted = 88 + 76 + 56 \rightarrow\\
    & 3 \time Predicted = 88 + 76 + 56 \rightarrow\\
    & \frac{3 \time Predicted}{3} = \frac{88 + 76 + 56}{3}\\
    & Predicted = 73
\end{align*}

So, given the loss function $\frac{1}{2}(Observed - Predicted)^2$ the value for $\gamma$ that minimizes the sum $\sum L(y_{i}, \gamma)$ is the $Predicted = \frac{88 + 76 + 56}{3}$, i.e the average of the observed weights. So, as you can see, we have now created the initial predicted value:
\begin{align*}
    F_{0}(x) = \frac{88 + 76 + 56}{3} = 73.3
\end{align*}

That means that the initial Predicted Value is just a  leaf, 73.3. The leaf Predicts that all samples will weight 73.3.

\subsubsection{Step 2}

\paragraph{Step 2:} for m = 1 to M:

\begin{itemize}
    \item \textbf{(A)} Compute $r_{im} = -[\frac{\partial L(y_{i}, F(x_{i}))}{\partial F(x_{i})}]_{F(x) = F_{m - 1^(x)}}$ for i = 1,...,n
    
    \item \textbf{(B)} Fit a regression tree to the $r_{im}$ values and create terminal regions $R_{jm}$, for j = 1...$j_{m}$
    
    \item For j = 1...$j_{m}$ compute $\gamma_{jm} = argmin \sum ;(y_{i}, F_{m - 1^(x_{i})} + \gamma)$
    
    \item \textbf{D Update} $F_{m^(x_{i})} = F_{m -1^(x)} + \nu \sum \gamma_{m^(I)}(X \amR R_{jm})$
\end{itemize}

As you can see step 2 is a loop where we make all of the trees, i.e for m = 1 to M. In generic terms, we will make M trees, but in practise most people set M = 100 and make 100 trees. Little $m$ refers to an individual tree. So when little m = 1 we are talking abou the first tree. When m = M, then we are talking about the last tree. Since, we are just starting Step 2, we set little m = 1.

\paragraph{Part A of step 2:} \textbf{(A)} Compute $r_{im} = -[\frac{\partial L(y_{i}, F(x_{i}))}{\partial F(x_{i})}]_{F(x) = F_{m - 1^(x)}}$ for i = 1,...,n looks a little bit confused but it's not. As you can see the part $[\frac{\partial L(y_{i}, F(x_{i}))}{\partial F(x_{i})}]_{F(x) =  F_{m - 1^(x)}}$ is just the derivative of Loss Function $\frac{\partial}{\partial Predicted}\frac{1}{2}(Observed - Predicted)^2$ with respect to the predicted value... and we have already calculated as = $-(Observed - Predicted)$. The minus sign (-) tells us to multiply the derivative by -1 : (-1 $\times -(Observed - Predicted)$) and that leaves us with the Observed value minus the predicted value (Observed - Predicted). In other words, this thing $-[\frac{\partial L(y_{i}, F(x_{i}))}{\partial F(x_{i})}]_{F(x) = F_{m - 1^(x)}}$ is just a \textbf{Residual}. Now we plug $F(x) =  F_{m - 1^(x)}$ in for Predicted, i.e \textbf{(Observed - $F_{m - 1^(x)}$)} and since m = 1 that means we plug in $F_{0}(x)$ for $F_{m - 1^(x)}$, i.e \textbf{(Observed - $F_{0}(x)$)} and since $F_{0}(x)$ = 73.3, we plug in 73.3.
\newline

Now we can compute $r_{i,m}$, where \textbf{r} is short for \textbf{Residual}, \tetxbf{i} is the sample number and \textbf{m} is the tree that we are trying to build. This \textbf{for i = 1...n} tells us to calculate \textbf{Residuals} for all three samples in the dataset(in our experiment the number of samples is equal to 3). So we well start with $r_{1, 1}$, the \textbf{Residual} for the first sample (i = 1) and the first tree (m = 1):
\begin{align*}
    & r_{1, 1} = (Observed - 73.3) \Rightarrow (88 - 73.3) = \textbf{14.7}\\
    & r_{2, 1} = (Observed - 73.3) \Rightarrow (76 - 73.3) = \textbf{2.7}\\
    & r_{3, 1} = (Observed - 73.3) \Rightarrow (56 - 73.3) = \textbf{-17.3}
\end{align*}

So, we finished Part A of step 2 by calculating a residual for each sample.

\textbf{NOTE:} An important note is that this derivative $[\frac{\partial L(y_{i}, F(x_{i}))}{\partial F(x_{i})}]$ is the \textbf{Gradient} that \textbf{Gradient Boost} is named after. Also, should be emphasized that $r_{i, m}$ values are technically called \textbf{Pseudo Residuals}. When we use this \textbf{Loss Function} $\Rightarrow$ $\frac{1}{2}(Observed - Predicted)^2$ we end up by calculating normal Residuals $\Rightarrow$ \textbf{(Observed - Predicted)}, but if we used another Loss Function $\Rightarrow$ \textbf{(Observed - Predicted)$^2$} we do not multiply by $\frac{1}{2}$ then we will end up with something similar to the Residuals, but not quite $\Rightarrow$ \textbf{2(Observed - Predicted)}, in other words we end up with \textbf{Pseudo Residual} and that is why $r_{i, m}$ called \textbf{Pseudo Residuals}.


\paragraph{Part B of Step 2:} \textbf{(B)} Fit a regression tree to the $r_{im}$ values and create terminal regions $R_{jm}$, for j = 1...$j_{m}$. All this saying that we will build a regression tree to Predict the \textbf{Residuals} instead of \textbf{Weights}. So we will use [\textbf{Height, Fvorite Color and Gender}] to predict the Residuals which have been calculated by Part A.
\newline

The new tree:

if \textbf{Height $<$ 1.55} in the first node assigned the residual with Value \textbf{17.3}.

else \textbf{Height $>$ 1.55} in the second node assigned the residuals with values \textbf{14.7, 2.7} and we will take their average $\frac{14.7 + 2.7}{2}$. \textbf{NOTE:} In Gradient Boost we do not use stump trees, but here we do this for purposes of demonstration. So, we have a regression tree fit to the residuals.

Now we need to create terminal regions $R_{j, m} for j = 1...j_{m}$. In this example, leaves are the terminal Regions. Little m is index of the tree that have been just made. Also, little j is the index for each leaf in the tree. Since this tree have 2 leaves $j_{m} = 2$. So:
\begin{align*}
    & -17.3 = R_{1, 1}\\
    & 14.7, 2.7 = R_{2, 1}\\
\end{align*}

So, as you can see we have completed the step 2 , fitting a regression in tree in initial Residuals and labeling the leaves.

\paragraph{Part C of Step 2:} For j = 1...$j_{m}$ compute $\gamma_{jm} = argmin \sum ;(y_{i}, F_{m - 1^(x_{i})} + \gamma)$. In this part we determine the output values for each leaf. Specifically since in the one out of 2 leaves ended up with 2 Residual Values is not clear what \textbf{Output Value} should be, i.e in leaf with residual 14.7 and 2.7.

So For j = 1...$j_{m}$, for each leaf in the new tree, we compute an output value, $\gamma_{j, m}$. The output value for each leaf is the value for $\gamma$ that minimizes this summation $\gamma_{jm} = argmin \sum L(y_{i}, F_{m - 1^(x_{i})} + \gamma)$. This summation is like what we did in \textbf{Step 1.} $F_{0}(x) = argmin \sum L(y_{i}, F_{m - 1^(x_{i})} + \gamma)$. One small difference is that now we are taking the previous \textbf{Prediction} into account $L(y_{i}, F_{m - 1^(x_{i})} + \gamma)$ while before, since we just starting out, there was no previous prediction $L(y_{i}, \gamma)$. The second difference is that this summation is picky about which sample it includes, while before the summation included all of the samples.Specifically the $x_{i} in R_{i, j}$ means that since only one sample $x_{3}$ goes to leaf $R_{1, 1}$ then only $x_{3}$ is used to to calculate the output value for $R_{i, j}$. Also, since only two samples $x_{1}$ and $x_{2}$ go to the leaf $R_{1, 2}$ then only $x_{1}$ and $x_{2}$ are used to calculate the output value for for $R_{1, 2}$.

Let's start by Calculating the Output Value for the leaf $R_{1, 1}$, $\gamma_{jm} = argmin \sum L(y_{i}, F_{m - 1^(x_{i})} + \gamma)$. This means that we have j = 1, i.e the first leaf and m =1 , since refers to the first leaf. Also we should to replace the generic Loss Function with the actual loss function that we decide to use $\gamma_{1,1} = argmin \sum \frac{1}{2}(y_{i} - F_{m - 1^(x_{i})} + \gamma )^2$... and let's expand this summation into individual terms. Since in the leaf $R_{1, 1}$ just one value has been assigned, expanding means we remove the big sigma and swap the i's with 3's, i.e  $\gamma_{1,1} = argmin  \frac{1}{2}(y_{3} - F_{m - 1^(x_{3})} + \gamma )^2$. So, 
\begin{align*}
    & \gamma_{1, 1} = argmin \frac{1}{2}(56 - (73.3 + \gamma))^2 \Rightarrow \\
    & argmin \frac{1}{2}(-17.3 + \gamma)^2\\
\end{align*}

Now, we need to find the value for $\gamma$ that minimizes the above equation:

\begin{align*}
    & \frac{\partial}{\partial \gamma} \frac{1}{2}(-17.3 + \gamma)^2 \Rightarrow\\
    & -(-17.3 + \gamma) \Rightarrow 17.3 + \gamma\\
    & 17.3 + \gamma = 0 \Rightarrow\\
    & \gamma = -17.3\\
\end{align*}

So, the value for $\gamma$ that minimizes this equation is -17.3 and that means that $\gamma_{1, 1}$ = -17.3 and ultimately the leaf $R_{1, 1}$, has an output value of -17.3.
Now let's solve for the $R_{2, 1}$: This refers to the j = 2, i.e the second leaf for the tree = 1. So:
\begin{align*}
    & \gamma_{2, 1} = [\frac{1}{2}(88 - 73.3 +   \gamma)^2] + [\frac{1}{2}(76 - 73.3 + \gamma)^2] \Rightarrow\\
    & [\frac{1}{2}(14.7 + \gamma)^2] + [\frac{1}{2}(2.7 + \gamma)^2] \Rightarrow\\
    & [(-14.7 + \gamma)] + [(-2.7 + \gamma)] = 0\\
    & \frac{2 \gamma}{2} = \frac{17.4}{2} \Rightarrow\\
    & \gamma_{2, 1} = \textbf{8.7}
\end{align*}

So, ultimately, the leaf $R_{2, 1}$ has an output Value of 8.7.

\paragraph{Part D of Step 2:} \textbf{D Update} $F_{m^(x_{i})} = F_{m -1^(x)} + \nu \sum \gamma_{m^(I)}(X \amR R_{jm})$. In Part D we make a new prediction for each sample. The above equation means that the new prediction $F_{1}(X)$ is based on the previous prediction $F_{0}(x)$ scaling the tree with a number $\nu$(in this example 0.1) the Learning rate that reduces the effect each tree has on the final prediction and this improves accuracy long run. 

Now, we will use $F_{1}(x)$ to make new Predictions for each sample:

\begin{enumerate}

\item \textbf{For X_{i}:} 73.3 + 0.1 $\times$ 8.7 = 74.2

\item \textbf{For X_{2}:} 73.3 + 0.1 $\times$ 8.7 = 74.2

\item \textbf{For X_{3}:} 73.3 + 0.1 $\times$ -17.3 = 71.6

\end{enumerate}

\paragraph{LAST NOTES:} Gradient Boost usually uses tree largest than stumps.
%---------------------------------------------------------------------------------------------------------------------
% Tables
%---------------------------------------------------------------------------------------------------------------------

\section{Gradient Boost for Classification}

As you can understand, Gradient Boost can be also used for classification and it has a lot in common with Logistic Regression. So, below you can see a synthetic dataset that will be utilized in order to explain what behind the scenes happens when we use Gradient Boost for classification.

\begin{table}[h!]
\begin{center}
\begin{tabular}{m|l|c|r} % <-- Alignments
    
    \hline
    \hline
    
         \textbf{Likes Popcorn}
     
       & \textbf{Age}
      
       & \textbf{Favorite Color}
       
       & \textbf{Loves Troll 2}
       
      \\
      
      \hline
      \hline
      
      Yes & 12  & Blue  & Yes\\
      Yes & 87  & Green & Yes\\
      No  & 44  & Blue  & No\\
      Yes & 19  & Red   & No\\
      No  & 32  & Green & Yes\\
      No  & 14  & Blue  & Yes\\
      
      \hline
      \hline
      
\end{tabular}
\end{center}
\end{table}

In this step, it could be useful to know that we start with a leaf that represents an initial Prediction for every individual. When we use \textbf{Gradient Boost} for Classification the initial Prediction for every individual is the \textbf{log(odds)}. Also, it could be useful to think log of the odds as the Logistic Regression equivalent of the average.

Thus, let's calculate the log of the odds that someone loves Troll 2. Since 4 people in the training set loves Troll 2 and specifically $x_{1}, x_{2}, x_{5}, x_{6}$ and 2 people do not, i.e $x_{3} x_{4}$ then the log of the odds that someone Loves Troll 2 is:
\begin{align*}
    & log(\frac{4}{2}) = \textbf{0.7}\\
\end{align*}

...which we will put it into initial leaf. So this is the initial Prediction. But, how we use it for classification. Just, like Logit Regression the easiest way to use logg(odds) for Classification is to convert it to a probability. This action will be implemented using \textbf{Logistic Function}. So the probability that someone loves Troll 2 is:
\begin{align*}
    & \frac{e^log(odds)}{1 + e^log(odds)} \Rightarrow\\
    & \frac{e^0.7}{1 + e^0.7} = 0.7
\end{align*}

So doing the math we get 0.7 as the Probability of someone loving Troll 2. Since the probability of someone loving Trolls 2 is 0.7, i.e greater than 0.5, we can classify everyone in the Training Set as someone who loves Troll 2. In addition, you should take into account that while 0.5 is a very common threshold for making Classification decisions based on Probability we could have just as easily used a different value.

In this step, we can measure how bad the initial Prediction is by calculating pseudo residuals. If you are wondering how its gonna happen, you can think that in y axis we have the predicted probability of Loving Troll 2 is 0.7. Also, in x axis you can thing that we have No and Yes of someone Loving Troll 2. So, we have 2 = No in 0 and 4 = Yes in 1. In other words, these are the observed values and the dotted line in y axis is the Predicted probability.So, for people who love Troll 2 we plug in the 1 as Yes, and 0 for people do not love Troll 2. Specifically, Residuals could be illustrated as below:
\begin{align*}
    & x{1}  = 1 - 0.7 = 0.3\\
    & x_{2} = 1 - 0.7 = 0.3\\
    & x_{3} = 0 - 0.7 = -0.7\\
    & x_{4} = 0 - 0.7 = -0.7\\
    & x_{5} = 1 - 0.7 = 0.3\\
    & x_{6} = 1 - 0.7 = 0.3\\
\end{align*}

Now, we build a tree to predict the Residuals. The tree has the below shape:
If someone's Color = Red then we assign the residual -0.7. If Someone does not love the red colour and his/her age is $>$ 37 then we assign in the second leaf residuals with values 0.3 and -0.7, but  if does not love red colour and his age is $<$ 37 then we assign in this leaf values 0.3 and 0.3 and 0.3. Now, let's calculate output values for the leaves. As you, possibly, remember when we use GB for Regression outputs values are equal to the residuals but in classification is more complex because the predictions are in terms of log(odds). So we cannot add them together and take a new log of the odds Predictions without some transformation. The most common transformation is. The numerator is the sum of all the residuals in the leaf, while the denominator is sum of the previously predicted probabilities for each Residual times 1 minus the same probability. The derivation is a little bit technical, so we will go deeper in Classification Details Chapter. So for now let just use the formula to calculate the output value for each leaf. An important thing that you should take into account is that for now the previous probas are the same, but when we build the sequential new tree the previous probas will change.
\begin{align*}
    & \frac{\sum Residual_{i}}{\sum Previous Probability_{i} \times (1 - Previous Probability_{i})}\\
    & \frac{ - 0.7}{0.7 \times (1 - 0.7)} = -3.3\\
    & \frac{- 0.4}{(0.7 \times (1 - 0.7)) + (0.7 \times (1 - 0.7))} = - 1\\
    & \frac{0.9}{(0.7 \times (1 - 0.7)) + (0.7 \times (1 - 0.7)) + (0.7 \times (1 - 0.7))} = 1.4\\
\end{align*}

Now, we will podate predictions adding learning rate in 0.8 just for illustration purposes and convert the new log prediction into Probability using the formula of Logit Function.
\begin{align*}
    & x_{1} = log(odds) = 0.7 + (0.8 \times 1.4) 1.8= \frac{e^1.8}{1 + e^1.8} = 0.9\\
    &.................................\\
\end{align*}

Just like before we calculate the residuals of the last predictions. So, now we have the residuals we build a new tree and we calculate the output values for each leaf using the formula as just before $\frac{\sum Residual_{i}}{\sum Previous Probability_{i} \times (1 - Previous Probability_{i})}$. Note that the previous Probability is not the initial probability but these that have been calculated building the first tree, scaling the tree by Learning Rate. So, the procedure is the same just before combining the new tree with the tree and the initial prediction that we have already calculate. Specifically:
\begin{enumerate}

    \item We started with a single leaf which is the initial Prediction for each individual.
    
    \item The we built a tree on the Residuals, the difference between the Observed values and the single value Predicted by the leaf.
    
    \item Then we calculates the Output Values for each leaf...
    
    \item ...and we scaled the tree by adding Learning Rate
    
    \item Then we built another tree based on the residuals, the difference between the Observed and Predicted probas by the leaf and the first tree...
    
    \item ...and we calculated the output values for each leaf of the second tree...
    
    \item ...and we scaled the new tree by adding Learning Rate.
    
    \item \textbf{This process repeats until we have made the maximum number of trees specified or the residuals get super small.}
\end{enumerate}


\textbf{NOTE:} GB uses trees with between 8 and 32 to leaves.


\section{Mathematics behind the Gradient Boost Algorithm for Classification}

Firstly should be emphasized that we will use a very small training dataset. The small size help us to focus on the algorithm's details, using stumps instead of larger trees.

\begin{table}[h!]
\begin{center}
\begin{tabular}{m|l|c|r} % <-- Alignments
    
    \hline
    \hline
    
         \textbf{Likes Popcorn}
     
       & \textbf{Age}
      
       & \textbf{Favorite Color}
       
       & \textbf{Loves Troll 2}
       
      \\
      
      \hline
      \hline
      
      Yes & 12  & Blue  & Yes\\
      Yes & 87  & Green & Yes\\
      No  & 44  & Blue  & No\\
      
      \hline
      \hline
      
\end{tabular}
\end{center}
\end{table}


As we can see that the  log likelihood of the (Observed) data given the predicted Probability. In this example the predicted probability is 0.67. This log likelihood of predicted Probability can be calculated by using the below summation:
\begin{align*}
    & - \sum_{i = 1}^{N} y_{i} \times log(p) + (1 - y_{i}) \times log(1 - p)\\
\end{align*}
Note the very important sign(minus) in front of summation.
\begin{itemize}
    \item The \textbf{p} refers to the predicted probability, which is 0.67 in this example.
    \item The $y_{i}$ refer to the observed values for Loves Troll 2. For the 2 people who love Troll 2, $y_{i}$ = 1, which means that the multiplication $(1 - y_{i}) \times log(1 - p)$ will be zero(0).
    \item In contrast for the one people who does not love Troll 2, $y_{i}$ = 0, which means that the term $y_{i} \times log(p)$ will also be zero(0), leaving just the $log(1 - p)$.
\end{itemize}

So, now let's use this summation in order to to calculate log(likelihood) of all three Observed values.

\begin{itemize}
    \item \textbf{Log(Likelihood) for the first person} who loves Troll 2, means that $y_{1}$ = 1. So:
    \begin{align*}
        & 1 \times log(0.67) + (1 - 1) \times log(1 - 0.67) = \textbf{log(0.67)}\\
    \end{align*}
    
    \item \textbf{Log(Likelihood) for the second Person is also} = log(0.67)
    
    \item \textbf{Log(Likelihood) for the third person will be:}
    \begin{align*}
        & 0 \times log(0.67) + (1 - 0) \times log(1 - 0.67) = \textbf{log(1 - 0.67)}\\
    \end{align*}
\end{itemize}

Note that the better the prediction, the largest the log likelihood and that is because when we apply Logistic Regression, the objective is to maximize log Likelihood. That means that if we want to use log Likelihood as a loss function, where smaller values represent better fitting models, then we need to multiply the log(Likelihood) by -1. So, we should again remember the negative sign if front of summation, a very important summation. Also, since a loss function sometimes deals with one sample at a time. So:
\begin{align*}
    & -[y_{i} \times log(p) + (1 - y_{i}) \times log(1 - p)]\\
\end{align*}

Now, it is a good idea to transform this equation, the negative Log Likelihood, so that it is a function of the predicted log(odds) instead of the predicted probability, \textbf{p} and we need also to simplify it. Thus:

\begin{align*}
    & -[y_{i} \times log(p) + (1 - y_{i}) \times log(1 - p)] \Rightarrow\\
    \end{align*}
The first that should be done is to distribute the minus sign through the equation.
    \begin{align*}
        & - y_{i} \times log(p) - (1 - y_{i} \times log(1 - p)) \Rightarrow\\
    \end{align*}
So, now we multiply $ - (1 - y_{i} \times log(1 - p))$. Then we combine $- y_{i} \times log(p)$ with 
        \begin{align*}
    & -y_{i} \times log(p) -log(1 - p) + y_{i} \times log(1 - p) \Rightarrow\\
    & - y_{i} \times [log(p) - log(1 - p)] - log(1 - p) \Rightarrow\\
    & - y_{i} \times log(odds) - log(1 - p) \Rightarrow\\
    & - y{i} \times log(odds) + log(1 + e^{log(odds)})\\
\end{align*}

So, we converted the negative log Likelihood of the data, which is a function of the predicted probability, \textbf{p}. So the loss function is $- y{i} \times log(odds) + log(1 + e^{log(odds)})$. Now its time to show that is differentiable.
\begin{align*}
    & - y{i} \times log(odds) + log(1 + e^{log(odds)})\\
    & \frac{\partial}{\partial log(odds)} - y{i} \times log(odds) + log(1 + e^{log(odds)})\\
    & 
\end{align*}

So, loss Function is:
\begin{align*}
    & - y_{i} \times log(odds) + log(1 + e^{log(odds)})\\
\end{align*}
As you can see loss function is just a transformation of the negative log(Likelihood). A differentiable loss function. So the derivative can be a function  of the predicted log(odds):
\begin{align*}
    & \frac{\partial}{\partial \log(odds)} = -y_{i} + \frac{e^{\log(odds)}}{1 + e^{\log(odds)}}\\
\end{align*}

or a function of predicted probability, p.
\begin{align*}
    & -y_{i} + \textbf{p}\\
\end{align*}


\paragraph{Step 1:} In this step we initialize model with a constant value : $F_{0}(X) = argmin_{\gamma} \sum^{N}_{i = 1} L(y_{i}, \gamma)$

Just like Gradient Boost in Regression we need to come up with an initial Prediction, using the above equation. The summation of $-y_{i} \times log(odds) + log(1 + e^{\log(odds)}) \Rightarrow \frac{\partial}{\partial \log(odds)} = -y_{i} + \frac{e^{\log(odds)}}{1 + e^{\log(odds)}}$.

So, let's take the derivative of each term with respect to the log(odds).
\begin{align*}
    & - 1 \times \log(odds) + \log(1 + e^{\log(odds)}) \Rightarrow -1 \frac{e^{\log(odds)}}{1 + e^{\log(odds)}}\\
    & -1 + p + -1 + p + (-0) + p = 0\\
    & \frac{3p}{3} = \frac{2}{3}\\
    & p = \frac{2}{3}\\
    & \log(odds) = \log(\frac{p}{1 - p}) \Rightarrow \log(\frac{\frac{2}{3}}{1 - \frac{2}{3}}) = \log(\frac{2}{1})
\end{align*}

So, given the above loss function the log(odds) for $\gamma$ that minimizes the summation is $\log(2/1)$. So:
\begin{align*}
    & F_{0}(x) = \log(\frac{2}{1}) \Rightarrow 0.69\\
\end{align*}
%---------------------------------------------------------------------------------------------------------------------
% List
%---------------------------------------------------------------------------------------------------------------------



%---------------------------------------------------------------------------------------------------------------------
% Bibliography
%---------------------------------------------------------------------------------------------------------------------

\newpage
\bibliography{Report-Template} % .bib file name. Do not add ".bib"
\bibliographystyle{IEEEtr} % Bibliography style. Find more styles at
% https://www.sharelatex.com/learn/Bibtex_bibliography_styles

%If you include bibliography without a single reference inside the document, the TeX file will not compile.

\end{document}